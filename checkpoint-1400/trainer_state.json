{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 1400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14388489208633093,
      "grad_norm": 2.019472122192383,
      "learning_rate": 0.0001987142857142857,
      "loss": 2.9603,
      "step": 10
    },
    {
      "epoch": 0.28776978417266186,
      "grad_norm": 2.4402055740356445,
      "learning_rate": 0.0001972857142857143,
      "loss": 2.5876,
      "step": 20
    },
    {
      "epoch": 0.4316546762589928,
      "grad_norm": 2.163235664367676,
      "learning_rate": 0.00019585714285714288,
      "loss": 2.4846,
      "step": 30
    },
    {
      "epoch": 0.5755395683453237,
      "grad_norm": 2.2838480472564697,
      "learning_rate": 0.00019442857142857144,
      "loss": 2.3402,
      "step": 40
    },
    {
      "epoch": 0.7194244604316546,
      "grad_norm": 1.9822134971618652,
      "learning_rate": 0.000193,
      "loss": 2.2587,
      "step": 50
    },
    {
      "epoch": 0.8633093525179856,
      "grad_norm": 1.9088237285614014,
      "learning_rate": 0.0001915714285714286,
      "loss": 2.2504,
      "step": 60
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.4730687141418457,
      "learning_rate": 0.00019014285714285715,
      "loss": 2.2312,
      "step": 70
    },
    {
      "epoch": 1.143884892086331,
      "grad_norm": 2.1159956455230713,
      "learning_rate": 0.00018871428571428574,
      "loss": 2.1225,
      "step": 80
    },
    {
      "epoch": 1.2877697841726619,
      "grad_norm": 1.9444382190704346,
      "learning_rate": 0.0001872857142857143,
      "loss": 2.1256,
      "step": 90
    },
    {
      "epoch": 1.4316546762589928,
      "grad_norm": 2.1999716758728027,
      "learning_rate": 0.00018585714285714286,
      "loss": 2.1803,
      "step": 100
    },
    {
      "epoch": 1.5755395683453237,
      "grad_norm": 2.263414144515991,
      "learning_rate": 0.00018442857142857144,
      "loss": 2.0934,
      "step": 110
    },
    {
      "epoch": 1.7194244604316546,
      "grad_norm": 1.9451484680175781,
      "learning_rate": 0.000183,
      "loss": 2.0918,
      "step": 120
    },
    {
      "epoch": 1.8633093525179856,
      "grad_norm": 2.25960636138916,
      "learning_rate": 0.00018157142857142856,
      "loss": 2.062,
      "step": 130
    },
    {
      "epoch": 2.0,
      "grad_norm": 4.282738208770752,
      "learning_rate": 0.00018014285714285715,
      "loss": 2.1202,
      "step": 140
    },
    {
      "epoch": 2.143884892086331,
      "grad_norm": 2.059870481491089,
      "learning_rate": 0.0001787142857142857,
      "loss": 1.9915,
      "step": 150
    },
    {
      "epoch": 2.287769784172662,
      "grad_norm": 2.3578290939331055,
      "learning_rate": 0.0001772857142857143,
      "loss": 2.0118,
      "step": 160
    },
    {
      "epoch": 2.431654676258993,
      "grad_norm": 2.610584259033203,
      "learning_rate": 0.00017585714285714288,
      "loss": 1.9811,
      "step": 170
    },
    {
      "epoch": 2.5755395683453237,
      "grad_norm": 2.1125617027282715,
      "learning_rate": 0.00017442857142857142,
      "loss": 1.9834,
      "step": 180
    },
    {
      "epoch": 2.7194244604316546,
      "grad_norm": 2.2790732383728027,
      "learning_rate": 0.000173,
      "loss": 2.0325,
      "step": 190
    },
    {
      "epoch": 2.8633093525179856,
      "grad_norm": 2.239048957824707,
      "learning_rate": 0.0001715714285714286,
      "loss": 1.9863,
      "step": 200
    },
    {
      "epoch": 3.0,
      "grad_norm": 3.3101398944854736,
      "learning_rate": 0.00017014285714285715,
      "loss": 1.9774,
      "step": 210
    },
    {
      "epoch": 3.143884892086331,
      "grad_norm": 2.1602866649627686,
      "learning_rate": 0.0001687142857142857,
      "loss": 1.9311,
      "step": 220
    },
    {
      "epoch": 3.287769784172662,
      "grad_norm": 2.749411106109619,
      "learning_rate": 0.0001672857142857143,
      "loss": 1.9135,
      "step": 230
    },
    {
      "epoch": 3.431654676258993,
      "grad_norm": 2.2835516929626465,
      "learning_rate": 0.00016585714285714286,
      "loss": 1.8577,
      "step": 240
    },
    {
      "epoch": 3.5755395683453237,
      "grad_norm": 2.4370641708374023,
      "learning_rate": 0.00016442857142857144,
      "loss": 1.9833,
      "step": 250
    },
    {
      "epoch": 3.7194244604316546,
      "grad_norm": 2.4575774669647217,
      "learning_rate": 0.000163,
      "loss": 1.8777,
      "step": 260
    },
    {
      "epoch": 3.8633093525179856,
      "grad_norm": 2.1299924850463867,
      "learning_rate": 0.00016157142857142856,
      "loss": 1.867,
      "step": 270
    },
    {
      "epoch": 4.0,
      "grad_norm": 3.468665361404419,
      "learning_rate": 0.00016014285714285715,
      "loss": 1.907,
      "step": 280
    },
    {
      "epoch": 4.143884892086331,
      "grad_norm": 2.733818769454956,
      "learning_rate": 0.00015871428571428574,
      "loss": 1.8211,
      "step": 290
    },
    {
      "epoch": 4.287769784172662,
      "grad_norm": 2.3988735675811768,
      "learning_rate": 0.0001572857142857143,
      "loss": 1.854,
      "step": 300
    },
    {
      "epoch": 4.431654676258993,
      "grad_norm": 2.7242825031280518,
      "learning_rate": 0.00015585714285714286,
      "loss": 1.7776,
      "step": 310
    },
    {
      "epoch": 4.575539568345324,
      "grad_norm": 2.5309603214263916,
      "learning_rate": 0.00015442857142857145,
      "loss": 1.8815,
      "step": 320
    },
    {
      "epoch": 4.719424460431655,
      "grad_norm": 2.461883306503296,
      "learning_rate": 0.000153,
      "loss": 1.7937,
      "step": 330
    },
    {
      "epoch": 4.863309352517986,
      "grad_norm": 2.8804736137390137,
      "learning_rate": 0.0001515714285714286,
      "loss": 1.8571,
      "step": 340
    },
    {
      "epoch": 5.0,
      "grad_norm": 3.7682366371154785,
      "learning_rate": 0.00015014285714285715,
      "loss": 1.8229,
      "step": 350
    },
    {
      "epoch": 5.143884892086331,
      "grad_norm": 2.8389010429382324,
      "learning_rate": 0.0001487142857142857,
      "loss": 1.795,
      "step": 360
    },
    {
      "epoch": 5.287769784172662,
      "grad_norm": 2.662964105606079,
      "learning_rate": 0.0001472857142857143,
      "loss": 1.7118,
      "step": 370
    },
    {
      "epoch": 5.431654676258993,
      "grad_norm": 2.662477731704712,
      "learning_rate": 0.00014585714285714286,
      "loss": 1.7731,
      "step": 380
    },
    {
      "epoch": 5.575539568345324,
      "grad_norm": 2.7332475185394287,
      "learning_rate": 0.00014442857142857145,
      "loss": 1.7749,
      "step": 390
    },
    {
      "epoch": 5.719424460431655,
      "grad_norm": 2.4938158988952637,
      "learning_rate": 0.000143,
      "loss": 1.761,
      "step": 400
    },
    {
      "epoch": 5.863309352517986,
      "grad_norm": 2.8098790645599365,
      "learning_rate": 0.00014157142857142857,
      "loss": 1.7569,
      "step": 410
    },
    {
      "epoch": 6.0,
      "grad_norm": 4.431039810180664,
      "learning_rate": 0.00014014285714285715,
      "loss": 1.7743,
      "step": 420
    },
    {
      "epoch": 6.143884892086331,
      "grad_norm": 2.7024428844451904,
      "learning_rate": 0.00013871428571428574,
      "loss": 1.6702,
      "step": 430
    },
    {
      "epoch": 6.287769784172662,
      "grad_norm": 2.9839587211608887,
      "learning_rate": 0.00013728571428571427,
      "loss": 1.6828,
      "step": 440
    },
    {
      "epoch": 6.431654676258993,
      "grad_norm": 2.849816083908081,
      "learning_rate": 0.00013585714285714286,
      "loss": 1.7587,
      "step": 450
    },
    {
      "epoch": 6.575539568345324,
      "grad_norm": 2.950178861618042,
      "learning_rate": 0.00013442857142857145,
      "loss": 1.7373,
      "step": 460
    },
    {
      "epoch": 6.719424460431655,
      "grad_norm": 2.8996505737304688,
      "learning_rate": 0.000133,
      "loss": 1.6611,
      "step": 470
    },
    {
      "epoch": 6.863309352517986,
      "grad_norm": 2.969715118408203,
      "learning_rate": 0.00013157142857142857,
      "loss": 1.7323,
      "step": 480
    },
    {
      "epoch": 7.0,
      "grad_norm": 4.902666091918945,
      "learning_rate": 0.00013014285714285715,
      "loss": 1.7075,
      "step": 490
    },
    {
      "epoch": 7.143884892086331,
      "grad_norm": 2.6596360206604004,
      "learning_rate": 0.00012871428571428571,
      "loss": 1.6979,
      "step": 500
    },
    {
      "epoch": 7.287769784172662,
      "grad_norm": 2.859907865524292,
      "learning_rate": 0.0001272857142857143,
      "loss": 1.6655,
      "step": 510
    },
    {
      "epoch": 7.431654676258993,
      "grad_norm": 3.0842885971069336,
      "learning_rate": 0.00012585714285714286,
      "loss": 1.6628,
      "step": 520
    },
    {
      "epoch": 7.575539568345324,
      "grad_norm": 3.0115981101989746,
      "learning_rate": 0.00012442857142857142,
      "loss": 1.6784,
      "step": 530
    },
    {
      "epoch": 7.719424460431655,
      "grad_norm": 2.690920114517212,
      "learning_rate": 0.000123,
      "loss": 1.6069,
      "step": 540
    },
    {
      "epoch": 7.863309352517986,
      "grad_norm": 3.026801347732544,
      "learning_rate": 0.00012157142857142858,
      "loss": 1.6743,
      "step": 550
    },
    {
      "epoch": 8.0,
      "grad_norm": 4.776262283325195,
      "learning_rate": 0.00012014285714285716,
      "loss": 1.6098,
      "step": 560
    },
    {
      "epoch": 8.14388489208633,
      "grad_norm": 2.999415159225464,
      "learning_rate": 0.00011871428571428572,
      "loss": 1.5527,
      "step": 570
    },
    {
      "epoch": 8.287769784172662,
      "grad_norm": 3.2874815464019775,
      "learning_rate": 0.00011728571428571429,
      "loss": 1.6269,
      "step": 580
    },
    {
      "epoch": 8.431654676258994,
      "grad_norm": 2.9395360946655273,
      "learning_rate": 0.00011585714285714286,
      "loss": 1.5983,
      "step": 590
    },
    {
      "epoch": 8.575539568345324,
      "grad_norm": 2.992018461227417,
      "learning_rate": 0.00011442857142857144,
      "loss": 1.5831,
      "step": 600
    },
    {
      "epoch": 8.719424460431654,
      "grad_norm": 2.57244610786438,
      "learning_rate": 0.000113,
      "loss": 1.6359,
      "step": 610
    },
    {
      "epoch": 8.863309352517986,
      "grad_norm": 2.9651312828063965,
      "learning_rate": 0.00011157142857142857,
      "loss": 1.6242,
      "step": 620
    },
    {
      "epoch": 9.0,
      "grad_norm": 4.955917835235596,
      "learning_rate": 0.00011014285714285714,
      "loss": 1.6299,
      "step": 630
    },
    {
      "epoch": 9.14388489208633,
      "grad_norm": 3.4085206985473633,
      "learning_rate": 0.00010871428571428573,
      "loss": 1.5734,
      "step": 640
    },
    {
      "epoch": 9.287769784172662,
      "grad_norm": 3.11179518699646,
      "learning_rate": 0.0001072857142857143,
      "loss": 1.5922,
      "step": 650
    },
    {
      "epoch": 9.431654676258994,
      "grad_norm": 2.8977768421173096,
      "learning_rate": 0.00010585714285714285,
      "loss": 1.5534,
      "step": 660
    },
    {
      "epoch": 9.575539568345324,
      "grad_norm": 3.305227756500244,
      "learning_rate": 0.00010442857142857144,
      "loss": 1.5347,
      "step": 670
    },
    {
      "epoch": 9.719424460431654,
      "grad_norm": 3.319331169128418,
      "learning_rate": 0.00010300000000000001,
      "loss": 1.5919,
      "step": 680
    },
    {
      "epoch": 9.863309352517986,
      "grad_norm": 3.053617238998413,
      "learning_rate": 0.00010157142857142858,
      "loss": 1.5561,
      "step": 690
    },
    {
      "epoch": 10.0,
      "grad_norm": 5.408145904541016,
      "learning_rate": 0.00010014285714285714,
      "loss": 1.5242,
      "step": 700
    },
    {
      "epoch": 10.14388489208633,
      "grad_norm": 3.2629988193511963,
      "learning_rate": 9.871428571428572e-05,
      "loss": 1.5071,
      "step": 710
    },
    {
      "epoch": 10.287769784172662,
      "grad_norm": 3.1039531230926514,
      "learning_rate": 9.728571428571429e-05,
      "loss": 1.4744,
      "step": 720
    },
    {
      "epoch": 10.431654676258994,
      "grad_norm": 3.1778464317321777,
      "learning_rate": 9.585714285714285e-05,
      "loss": 1.5195,
      "step": 730
    },
    {
      "epoch": 10.575539568345324,
      "grad_norm": 3.152440071105957,
      "learning_rate": 9.442857142857144e-05,
      "loss": 1.4907,
      "step": 740
    },
    {
      "epoch": 10.719424460431654,
      "grad_norm": 3.4492781162261963,
      "learning_rate": 9.300000000000001e-05,
      "loss": 1.5067,
      "step": 750
    },
    {
      "epoch": 10.863309352517986,
      "grad_norm": 3.3371875286102295,
      "learning_rate": 9.157142857142857e-05,
      "loss": 1.551,
      "step": 760
    },
    {
      "epoch": 11.0,
      "grad_norm": 4.806384086608887,
      "learning_rate": 9.014285714285716e-05,
      "loss": 1.5936,
      "step": 770
    },
    {
      "epoch": 11.14388489208633,
      "grad_norm": 3.5742228031158447,
      "learning_rate": 8.871428571428572e-05,
      "loss": 1.5018,
      "step": 780
    },
    {
      "epoch": 11.287769784172662,
      "grad_norm": 3.448624610900879,
      "learning_rate": 8.728571428571429e-05,
      "loss": 1.5377,
      "step": 790
    },
    {
      "epoch": 11.431654676258994,
      "grad_norm": 3.58864426612854,
      "learning_rate": 8.585714285714286e-05,
      "loss": 1.4622,
      "step": 800
    },
    {
      "epoch": 11.575539568345324,
      "grad_norm": 3.5080020427703857,
      "learning_rate": 8.442857142857144e-05,
      "loss": 1.4456,
      "step": 810
    },
    {
      "epoch": 11.719424460431654,
      "grad_norm": 3.2349843978881836,
      "learning_rate": 8.3e-05,
      "loss": 1.4509,
      "step": 820
    },
    {
      "epoch": 11.863309352517986,
      "grad_norm": 3.413114547729492,
      "learning_rate": 8.157142857142857e-05,
      "loss": 1.5006,
      "step": 830
    },
    {
      "epoch": 12.0,
      "grad_norm": 5.298885822296143,
      "learning_rate": 8.014285714285715e-05,
      "loss": 1.4933,
      "step": 840
    },
    {
      "epoch": 12.14388489208633,
      "grad_norm": 3.1800293922424316,
      "learning_rate": 7.871428571428572e-05,
      "loss": 1.4653,
      "step": 850
    },
    {
      "epoch": 12.287769784172662,
      "grad_norm": 3.5823636054992676,
      "learning_rate": 7.728571428571429e-05,
      "loss": 1.4204,
      "step": 860
    },
    {
      "epoch": 12.431654676258994,
      "grad_norm": 3.7348098754882812,
      "learning_rate": 7.585714285714287e-05,
      "loss": 1.4961,
      "step": 870
    },
    {
      "epoch": 12.575539568345324,
      "grad_norm": 3.070763111114502,
      "learning_rate": 7.442857142857144e-05,
      "loss": 1.4047,
      "step": 880
    },
    {
      "epoch": 12.719424460431654,
      "grad_norm": 2.8931398391723633,
      "learning_rate": 7.3e-05,
      "loss": 1.4827,
      "step": 890
    },
    {
      "epoch": 12.863309352517986,
      "grad_norm": 3.0485541820526123,
      "learning_rate": 7.157142857142857e-05,
      "loss": 1.456,
      "step": 900
    },
    {
      "epoch": 13.0,
      "grad_norm": 5.291842937469482,
      "learning_rate": 7.014285714285715e-05,
      "loss": 1.4686,
      "step": 910
    },
    {
      "epoch": 13.14388489208633,
      "grad_norm": 3.70312762260437,
      "learning_rate": 6.871428571428572e-05,
      "loss": 1.4167,
      "step": 920
    },
    {
      "epoch": 13.287769784172662,
      "grad_norm": 3.4520959854125977,
      "learning_rate": 6.728571428571428e-05,
      "loss": 1.3981,
      "step": 930
    },
    {
      "epoch": 13.431654676258994,
      "grad_norm": 3.026446580886841,
      "learning_rate": 6.585714285714287e-05,
      "loss": 1.4497,
      "step": 940
    },
    {
      "epoch": 13.575539568345324,
      "grad_norm": 3.336881160736084,
      "learning_rate": 6.442857142857143e-05,
      "loss": 1.4454,
      "step": 950
    },
    {
      "epoch": 13.719424460431654,
      "grad_norm": 3.3180301189422607,
      "learning_rate": 6.3e-05,
      "loss": 1.4123,
      "step": 960
    },
    {
      "epoch": 13.863309352517986,
      "grad_norm": 3.5606350898742676,
      "learning_rate": 6.157142857142857e-05,
      "loss": 1.4511,
      "step": 970
    },
    {
      "epoch": 14.0,
      "grad_norm": 5.60369348526001,
      "learning_rate": 6.014285714285715e-05,
      "loss": 1.3877,
      "step": 980
    },
    {
      "epoch": 14.14388489208633,
      "grad_norm": 3.4980599880218506,
      "learning_rate": 5.871428571428572e-05,
      "loss": 1.4105,
      "step": 990
    },
    {
      "epoch": 14.287769784172662,
      "grad_norm": 3.5907161235809326,
      "learning_rate": 5.728571428571429e-05,
      "loss": 1.3774,
      "step": 1000
    },
    {
      "epoch": 14.431654676258994,
      "grad_norm": 3.543545961380005,
      "learning_rate": 5.585714285714286e-05,
      "loss": 1.3952,
      "step": 1010
    },
    {
      "epoch": 14.575539568345324,
      "grad_norm": 3.439953088760376,
      "learning_rate": 5.442857142857143e-05,
      "loss": 1.3766,
      "step": 1020
    },
    {
      "epoch": 14.719424460431654,
      "grad_norm": 3.029481887817383,
      "learning_rate": 5.300000000000001e-05,
      "loss": 1.3703,
      "step": 1030
    },
    {
      "epoch": 14.863309352517986,
      "grad_norm": 3.939645767211914,
      "learning_rate": 5.157142857142857e-05,
      "loss": 1.4436,
      "step": 1040
    },
    {
      "epoch": 15.0,
      "grad_norm": 5.329958438873291,
      "learning_rate": 5.014285714285715e-05,
      "loss": 1.402,
      "step": 1050
    },
    {
      "epoch": 15.14388489208633,
      "grad_norm": 3.4426090717315674,
      "learning_rate": 4.8714285714285714e-05,
      "loss": 1.3735,
      "step": 1060
    },
    {
      "epoch": 15.287769784172662,
      "grad_norm": 3.639855146408081,
      "learning_rate": 4.728571428571429e-05,
      "loss": 1.4252,
      "step": 1070
    },
    {
      "epoch": 15.431654676258994,
      "grad_norm": 3.566974401473999,
      "learning_rate": 4.585714285714286e-05,
      "loss": 1.3475,
      "step": 1080
    },
    {
      "epoch": 15.575539568345324,
      "grad_norm": 3.451734781265259,
      "learning_rate": 4.442857142857143e-05,
      "loss": 1.3821,
      "step": 1090
    },
    {
      "epoch": 15.719424460431654,
      "grad_norm": 3.9720842838287354,
      "learning_rate": 4.3e-05,
      "loss": 1.3523,
      "step": 1100
    },
    {
      "epoch": 15.863309352517986,
      "grad_norm": 3.659759759902954,
      "learning_rate": 4.1571428571428575e-05,
      "loss": 1.3911,
      "step": 1110
    },
    {
      "epoch": 16.0,
      "grad_norm": 5.86805534362793,
      "learning_rate": 4.014285714285714e-05,
      "loss": 1.3693,
      "step": 1120
    },
    {
      "epoch": 16.14388489208633,
      "grad_norm": 3.5848844051361084,
      "learning_rate": 3.8714285714285715e-05,
      "loss": 1.3404,
      "step": 1130
    },
    {
      "epoch": 16.28776978417266,
      "grad_norm": 3.3745362758636475,
      "learning_rate": 3.728571428571428e-05,
      "loss": 1.3179,
      "step": 1140
    },
    {
      "epoch": 16.431654676258994,
      "grad_norm": 3.945876121520996,
      "learning_rate": 3.585714285714286e-05,
      "loss": 1.3546,
      "step": 1150
    },
    {
      "epoch": 16.575539568345324,
      "grad_norm": 3.668565034866333,
      "learning_rate": 3.442857142857143e-05,
      "loss": 1.383,
      "step": 1160
    },
    {
      "epoch": 16.719424460431654,
      "grad_norm": 3.6543145179748535,
      "learning_rate": 3.3e-05,
      "loss": 1.3741,
      "step": 1170
    },
    {
      "epoch": 16.863309352517987,
      "grad_norm": 3.5263681411743164,
      "learning_rate": 3.1571428571428576e-05,
      "loss": 1.351,
      "step": 1180
    },
    {
      "epoch": 17.0,
      "grad_norm": 5.191943645477295,
      "learning_rate": 3.0142857142857146e-05,
      "loss": 1.3616,
      "step": 1190
    },
    {
      "epoch": 17.14388489208633,
      "grad_norm": 3.8464512825012207,
      "learning_rate": 2.8714285714285716e-05,
      "loss": 1.3522,
      "step": 1200
    },
    {
      "epoch": 17.28776978417266,
      "grad_norm": 3.3732495307922363,
      "learning_rate": 2.7285714285714286e-05,
      "loss": 1.3469,
      "step": 1210
    },
    {
      "epoch": 17.431654676258994,
      "grad_norm": 3.4819326400756836,
      "learning_rate": 2.5857142857142856e-05,
      "loss": 1.3351,
      "step": 1220
    },
    {
      "epoch": 17.575539568345324,
      "grad_norm": 3.5932819843292236,
      "learning_rate": 2.442857142857143e-05,
      "loss": 1.3352,
      "step": 1230
    },
    {
      "epoch": 17.719424460431654,
      "grad_norm": 3.78263521194458,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.3419,
      "step": 1240
    },
    {
      "epoch": 17.863309352517987,
      "grad_norm": 3.777204751968384,
      "learning_rate": 2.1571428571428574e-05,
      "loss": 1.3644,
      "step": 1250
    },
    {
      "epoch": 18.0,
      "grad_norm": 7.306759834289551,
      "learning_rate": 2.0142857142857144e-05,
      "loss": 1.308,
      "step": 1260
    },
    {
      "epoch": 18.14388489208633,
      "grad_norm": 3.615527868270874,
      "learning_rate": 1.8714285714285714e-05,
      "loss": 1.3175,
      "step": 1270
    },
    {
      "epoch": 18.28776978417266,
      "grad_norm": 3.8936541080474854,
      "learning_rate": 1.7285714285714287e-05,
      "loss": 1.3752,
      "step": 1280
    },
    {
      "epoch": 18.431654676258994,
      "grad_norm": 3.3598155975341797,
      "learning_rate": 1.5857142857142857e-05,
      "loss": 1.3423,
      "step": 1290
    },
    {
      "epoch": 18.575539568345324,
      "grad_norm": 3.7935125827789307,
      "learning_rate": 1.442857142857143e-05,
      "loss": 1.3284,
      "step": 1300
    },
    {
      "epoch": 18.719424460431654,
      "grad_norm": 3.770379066467285,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.2909,
      "step": 1310
    },
    {
      "epoch": 18.863309352517987,
      "grad_norm": 3.8282835483551025,
      "learning_rate": 1.1571428571428573e-05,
      "loss": 1.3148,
      "step": 1320
    },
    {
      "epoch": 19.0,
      "grad_norm": 6.205051422119141,
      "learning_rate": 1.0142857142857143e-05,
      "loss": 1.3271,
      "step": 1330
    },
    {
      "epoch": 19.14388489208633,
      "grad_norm": 3.538689613342285,
      "learning_rate": 8.714285714285715e-06,
      "loss": 1.2856,
      "step": 1340
    },
    {
      "epoch": 19.28776978417266,
      "grad_norm": 3.829948663711548,
      "learning_rate": 7.285714285714286e-06,
      "loss": 1.3545,
      "step": 1350
    },
    {
      "epoch": 19.431654676258994,
      "grad_norm": 3.6254959106445312,
      "learning_rate": 5.857142857142857e-06,
      "loss": 1.3387,
      "step": 1360
    },
    {
      "epoch": 19.575539568345324,
      "grad_norm": 3.4990668296813965,
      "learning_rate": 4.428571428571428e-06,
      "loss": 1.3057,
      "step": 1370
    },
    {
      "epoch": 19.719424460431654,
      "grad_norm": 3.5106570720672607,
      "learning_rate": 3e-06,
      "loss": 1.3113,
      "step": 1380
    },
    {
      "epoch": 19.863309352517987,
      "grad_norm": 3.448798179626465,
      "learning_rate": 1.5714285714285717e-06,
      "loss": 1.3186,
      "step": 1390
    },
    {
      "epoch": 20.0,
      "grad_norm": 5.829257011413574,
      "learning_rate": 1.4285714285714287e-07,
      "loss": 1.3251,
      "step": 1400
    }
  ],
  "logging_steps": 10,
  "max_steps": 1400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.06289080467456e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
